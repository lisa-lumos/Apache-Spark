# 3. Getting Started with Apache Spark
## Introduction to Spark Data Frames
Spark allow you to choose the file format, and it supports csv, json, parquet, avro, xml, etc. 

Spark dataframe is same as a table without a metadata store (schema info). The schema info stores in the runtime metadata catalog, because the Spark dataframe is a runtime (temporary) object, and it supports schema on read. 

You can use SQL on table, but Dataframe doesn't support SQL expressions - you must use dataframe APIs to process data from a dataframe. 

You can use SQL and dataframe API to process data with Spark. 

## Creating Spark Dataframe


## Creating Spark Tables


## Common problem with Databricks Community


## Working with Spark SQL


## Dataframe Transformations and Actions


## Applying Transformations


## Querying Spark Dataframe


## More Dataframe Transformations

















